fix_seed: 0
checkpoints_every: 256
tensorboard_log: True
tqdm_progress_bar: False
result_path: ./results/test
find_unused_parameters: False
dispatch_batches: False
even_batches: True
non_blocking: False
split_batches: False

resume:
  enabled: False
  resume_path: results/dgx/vqvae/2025-07-02__01-28-28/checkpoints/epoch_8.pth
  restart_optimizer: True
  discard_decoder_weights: False

model:
  compile_model: True
  max_length: 512
  decoder_output_scaling_factor: 1  # Added scaling factor for backbone prediction outputs
  use_ndlinear: False  # Toggle for using NdLinear instead of Conv1d layers
  encoder:
    name: gcpnet # gcpnet
    freeze_parameters: False
    pretrained:
      enabled: True
      checkpoint_path: ./models/checkpoints/structure_denoising/gcpnet/ca_bb/last.ckpt # define your checkpoint directory here
  vqvae:
    vector_quantization:
      enabled: True
      freeze_parameters: False
      dim: 128
      decay: 0.99
      codebook_size: 4096
      commitment_weight: 0.25
      orthogonal_reg_weight: 10
      orthogonal_reg_max_codes: 512
      orthogonal_reg_active_codes_only: True
      rotation_trick: True
      threshold_ema_dead_code: 2
      kmeans_init: True
      kmeans_iters: 10
      stochastic_sample_codes: False # Doesn't work well with rotation trick
      sample_codebook_temp: 0.1
      alpha: 1.0
      tik_tok:
        enabled: True
        compression_factor: 8
        residual_depth: 1
        classifier_weight: 0.1
        adaptive_coefficient: True
    encoder:
      freeze_parameters: False
      dimension: 1024  # Used as dim_in and dim_out for ContinuousTransformerWrapper
      ff_mult: 4         # Multiplier for the feedforward dimension
      depth: 8      # Number of layers in the Encoder
      heads: 8      # Number of attention heads in the Encoder
      rotary_pos_emb: True
      attn_flash: True # FA-2 if installed
      attn_kv_heads: 2 # GQA
      qk_norm: True
      pre_norm: True
      residual_attn: False # Set pre_norm to False if residual_attn is True
      num_memory_tokens: 0 # Number of memory tokens, 0 means no memory tokens
      causal: False # Enable causal masking for autoregressive attention
    decoder:
      name: geometric_decoder # geometric_decoder
      freeze_parameters: False

train_settings:
  data_path: ../../datasets/vqvae/uniref_50/
  num_epochs: 32
  sample_weighting:
    enabled: True
    min_weight: 1.0
    max_weight: 4.0
    threshold_length: 384
  shuffle: True
  mixed_precision: bf16 # no, fp16, bf16, fp8
  save_pdb_every: 8
  batch_size: 32
  num_workers: 8
  grad_accumulation: 1
  max_task_samples: 100000
  profile_train_loop: False
  cutoff_augmentation:
    enabled: False
    probability: 0.5
    min_length: 25
  nan_augmentation:
    enabled: True
    probability: 0.05
    max_length: 50
  gaussian_jitter:           # Added augmentation block
    enabled: False           # Set True to enable Gaussian coordinate jitter
    std: 0.05                # Standard deviation in Angstroms for N(0, std^2) noise
    probability: 0.5         # Probability to apply jitter to a sample
  random_rotation:           # New rotation-only augmentation (global rigid rotation)
    enabled: True            # Applies a single random 3D rotation to all non-NaN backbone coords
  gradient_norm_logging_freq: 50  # How often to calculate and log gradient norm (in steps)
  log_separate_grad_norms: True  # Log gradient norms for each loss separately (disable when using model compilation)
  adaptive_loss_coefficient: True  # will enable only when log_separate_grad_norms is True
  losses:
    alignment_strategy: kabsch # kabsch, no
    mse:
      enabled: True
      weight: 0.001
      adaptive_coefficient: True
    backbone_distance:
      enabled: True
      weight: 0.01
      adaptive_coefficient: True
    backbone_direction:
      enabled: True
      weight: 0.05
      adaptive_coefficient: True
    binned_distance_classification:
      enabled: False
      weight: 0.01
      adaptive_coefficient: True
    binned_direction_classification:
      enabled: False
      weight: 0.01
      adaptive_coefficient: True
    next_token_prediction:
      enabled: True
      weight: 0.001
      adaptive_coefficient: True
      blocks: 0  # Number of transformer blocks, 0 means a linear layer only
    vq:
      adaptive_coefficient: True

valid_settings:
  data_path: ../../datasets/vqvae/whole_validation_2048_h5/
  do_every: 1
  save_pdb_every: 1
  batch_size: 8
  num_workers: 0

optimizer:
  name: adam
  lr: 1e-4
  weight_decouple: True
  weight_decay: 1e-2
  eps: 1e-7
  beta_1: 0.9
  beta_2: 0.98
  use_8bit_adam: True
  grad_clip_norm: 1.0
  decay:
    warmup: 4096
    min_lr: 1e-6
    gamma: 0.2
    num_restarts: 1
